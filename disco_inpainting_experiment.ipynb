{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Starting Run:\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import random\n",
    "import json\n",
    "import lpips\n",
    "import gc\n",
    "from secondary_model import SecondaryDiffusionImageNet2\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "import clip\n",
    "from types import SimpleNamespace\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from ipywidgets import Output\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /home/chenweifeng/anaconda3/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Prepping model...model name: custom\n",
      "Diffusion_model Loaded custom\n",
      "Prepping model...model name: CLIP\n",
      "CLIP Loaded\n"
     ]
    }
   ],
   "source": [
    "# LOADING MODEL\n",
    "custom_path = '/home/chenweifeng/image_generation_project/disco_project/models/nature_ema_160000.pt'\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "print(f'Prepping model...model name: {diffusion_model}')\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "if diffusion_model == 'custom':\n",
    "    model.load_state_dict(torch.load(custom_path, map_location='cpu'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{model_path}/{get_model_filename(diffusion_model)}', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "print(f'Diffusion_model Loaded {diffusion_model}')\n",
    "\n",
    "# NOTE Directly Load The Text Encoder From Hugging Face\n",
    "print(f'Prepping model...model name: CLIP')\n",
    "taiyi_tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\")\n",
    "taiyi_transformer = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\").eval().to(device)\n",
    "clip_models = []\n",
    "if ViTB32:\n",
    "    clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTB16:\n",
    "    clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14:\n",
    "    clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if ViTL14_336px:\n",
    "    clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "print(f'CLIP Loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Frame 0 Prompt: ['一颗紫色大树']\n",
      "torch.Size([1, 3, 512, 512]) init\n",
      "torch.Size([1, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1513595/2804155785.py:60: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
      "/tmp/ipykernel_1513595/2804155785.py:65: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  mask = mask.resize((args.side_x, args.side_y), Image.NEAREST)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10be63b05d54401db9115dc53d345a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88122c6366f5416e83fe3202eab753ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = seed\n",
    "frame_num = 0\n",
    "clip_guidance_scale = 5000\n",
    "init_image=Image.open(fetch('./sunset.jpg')).convert('RGB')\n",
    "mask_image = Image.open(fetch('/home/chenweifeng/image_editing_project/blended-diffusion/input_example/mask2.png')).convert('RGB')\n",
    "\n",
    "init_scale = 1000\n",
    "skip_steps = 10\n",
    "loss_values = []\n",
    "if seed is not None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "target_embeds, weights = [], []\n",
    "frame_prompt = ['一颗紫色大树']\n",
    "\n",
    "print(args.image_prompts_series)\n",
    "if args.image_prompts_series is not None and frame_num >= len(args.image_prompts_series):\n",
    "    image_prompt = args.image_prompts_series[-1]\n",
    "elif args.image_prompts_series is not None:\n",
    "    image_prompt = args.image_prompts_series[frame_num]\n",
    "else:\n",
    "    image_prompt = []\n",
    "\n",
    "print(f'Frame {frame_num} Prompt: {frame_prompt}')\n",
    "\n",
    "model_stats = []\n",
    "for clip_model in clip_models:\n",
    "    cutn = 16\n",
    "    model_stat = {\"clip_model\": None, \"target_embeds\": [], \"make_cutouts\": None, \"weights\": []}\n",
    "    model_stat[\"clip_model\"] = clip_model\n",
    "\n",
    "    for prompt in frame_prompt:\n",
    "        txt, weight = parse_prompt(prompt)\n",
    "        # txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "        # NOTE use chinese CLIP\n",
    "        txt = taiyi_transformer(taiyi_tokenizer(txt, return_tensors='pt')['input_ids'].to(device)).logits\n",
    "        if args.fuzzy_prompt:\n",
    "            for i in range(25):\n",
    "                model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0, 1))\n",
    "                model_stat[\"weights\"].append(weight)\n",
    "        else:\n",
    "            model_stat[\"target_embeds\"].append(txt)\n",
    "            model_stat[\"weights\"].append(weight)\n",
    "\n",
    "  \n",
    "    model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
    "    model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
    "    if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
    "        raise RuntimeError('The weights must not sum to 0.')\n",
    "    model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
    "    model_stats.append(model_stat)\n",
    "\n",
    "init = None\n",
    "if init_image is not None:\n",
    "    # init = Image.open(fetch(init_image)).convert('RGB')   # 传递的是加载好的图片。而非地址~\n",
    "    init = init_image\n",
    "    init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
    "    init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    \n",
    "    print(init.shape,'init')\n",
    "    mask = mask_image\n",
    "    mask = mask.resize((args.side_x, args.side_y), Image.NEAREST)\n",
    "    # print(mask.size)\n",
    "    image_mask_pil_binarized = ((np.array(mask) > 0.5) * 255).astype(np.uint8)\n",
    "    # print(image_mask_pil_binarized.shape)\n",
    "    mask = TF.to_tensor(Image.fromarray(image_mask_pil_binarized))\n",
    "    mask = mask[0, ...].unsqueeze(0).unsqueeze(0).to(device)\n",
    "    print(mask.shape)\n",
    "\n",
    "\n",
    "    if args.perlin_init:\n",
    "        # NOTE在原始图像上加perlin（柏林噪声）\n",
    "        if args.perlin_mode == 'color':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "        elif args.perlin_mode == 'gray':\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        else:\n",
    "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "        # init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
    "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "        del init2\n",
    "\n",
    "cur_t = None\n",
    "\n",
    "def cond_fn(x, t, y=None):\n",
    "    with torch.enable_grad():\n",
    "        x_is_NaN = False\n",
    "        x = x.detach().requires_grad_()\n",
    "        n = x.shape[0]\n",
    "        if use_secondary_model is True:\n",
    "            alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "            sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
    "            cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
    "            out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
    "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "            x_in = out * fac + x * (1 - fac)\n",
    "            x_in_grad = torch.zeros_like(x_in)\n",
    "        else:\n",
    "            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
    "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "            x_in_grad = torch.zeros_like(x_in)\n",
    "\n",
    "            if mask is not None:\n",
    "                # only calculate the gradient on the masked region\n",
    "                # torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512]) \n",
    "                # print(x_in.shape, mask.shape, ' >>>> x_in.shape, mask')\n",
    "                x_in = x_in * mask\n",
    "                # image = TF.to_pil_image(x_in[0].add(1).div(2).clamp(0, 1))\n",
    "                # if j % args.display_rate == 0 or cur_t == -1:\n",
    "                #     image.save(f'{outDirPath}/9999.jpg')\n",
    "\n",
    "        for model_stat in model_stats:\n",
    "            for i in range(args.cutn_batches):\n",
    "                t_int = int(t.item())+1  # errors on last step without +1, need to find source\n",
    "                # when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
    "                try:\n",
    "                    input_resolution = model_stat[\"clip_model\"].visual.input_resolution\n",
    "                except:\n",
    "                    input_resolution = 224\n",
    "\n",
    "                cuts = MakeCutoutsDango(input_resolution,\n",
    "                                        Overview=args.cut_overview[1000-t_int],\n",
    "                                        InnerCrop=args.cut_innercut[1000-t_int],\n",
    "                                        IC_Size_Pow=args.cut_ic_pow[1000-t_int],\n",
    "                                        IC_Grey_P=args.cut_icgray_p[1000-t_int],\n",
    "                                        args=args,\n",
    "                                        )\n",
    "                clip_in = normalize(cuts(x_in.add(1).div(2)))\n",
    "                image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n",
    "                dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
    "                dists = dists.view([args.cut_overview[1000-t_int]+args.cut_innercut[1000-t_int], n, -1])\n",
    "                losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
    "                loss_values.append(losses.sum().item())  # log loss, probably shouldn't do per cutn_batch\n",
    "                x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n",
    "        tv_losses = tv_loss(x_in)\n",
    "        if use_secondary_model is True:\n",
    "            range_losses = range_loss(out)\n",
    "        else:\n",
    "            range_losses = range_loss(out['pred_xstart'])\n",
    "        sat_losses = torch.abs(x_in - x_in.clamp(min=-1, max=1)).mean()\n",
    "        loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
    "        if init is not None and init_scale:\n",
    "            init_losses = lpips_model(x_in, init)\n",
    "            loss = loss + init_losses.sum() * init_scale\n",
    "        x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "        if torch.isnan(x_in_grad).any() == False:\n",
    "            grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "        else:\n",
    "            # print(\"NaN'd\")\n",
    "            x_is_NaN = True\n",
    "            grad = torch.zeros_like(x)\n",
    "    if args.clamp_grad and x_is_NaN == False:\n",
    "        magnitude = grad.square().mean().sqrt()\n",
    "        return grad * magnitude.clamp(max=args.clamp_max) / magnitude  # min=-0.02, min=-clamp_max,\n",
    "    return grad\n",
    "\n",
    "if args.diffusion_sampling_mode == 'ddim':\n",
    "    sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "else:\n",
    "    sample_fn = diffusion.plms_sample_loop_progressive\n",
    "\n",
    "for i in range(args.n_batches):\n",
    "    current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
    "    \n",
    "    batchBar = tqdm(range(args.n_batches), desc=\"Batches\")\n",
    "    batchBar.n = i\n",
    "    batchBar.refresh()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    cur_t = diffusion.num_timesteps - skip_steps - 1\n",
    "    total_steps = cur_t\n",
    "\n",
    "    if perlin_init:\n",
    "        init = regen_perlin(device)\n",
    "\n",
    "    if args.diffusion_sampling_mode == 'ddim':\n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            (batch_size, 3, args.side_y, args.side_x),\n",
    "            clip_denoised=clip_denoised,\n",
    "            model_kwargs={},\n",
    "            cond_fn=cond_fn,\n",
    "            progress=True,\n",
    "            skip_timesteps=skip_steps,\n",
    "            init_image=init,\n",
    "            randomize_class=randomize_class,\n",
    "            eta=eta,\n",
    "            transformation_fn=symmetry_transformation_fn,\n",
    "            transformation_percent=args.transformation_percent,\n",
    "            inpainting_mode=True,\n",
    "            mask_inpaint = mask\n",
    "        )\n",
    "    else:\n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            (batch_size, 3, args.side_y, args.side_x),\n",
    "            clip_denoised=clip_denoised,\n",
    "            model_kwargs={},\n",
    "            cond_fn=cond_fn,\n",
    "            progress=True,\n",
    "            skip_timesteps=skip_steps,\n",
    "            init_image=init,\n",
    "            randomize_class=randomize_class,\n",
    "            order=2,\n",
    "        )\n",
    "\n",
    "    for j, sample in enumerate(samples):\n",
    "        cur_t -= 1\n",
    "        intermediateStep = False\n",
    "        if args.steps_per_checkpoint is not None:\n",
    "            if j % steps_per_checkpoint == 0 and j > 0:\n",
    "                intermediateStep = True\n",
    "        elif j in args.intermediate_saves:\n",
    "            intermediateStep = True\n",
    "        if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n",
    "            for k, image in enumerate(sample['pred_xstart']):\n",
    "                # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                percent = math.ceil(j/total_steps*100)\n",
    "                if args.n_batches > 0:\n",
    "                    filename = f'{current_time}-{parse_prompt(prompt)[0]}.png'\n",
    "                # print(init.shape, mask.shape, image.shape)\n",
    "\n",
    "                pred_image = init[0] * (1-mask[0]) + image * mask[0] # 强制只修改mask区域，其它地方保存原样\n",
    "                image = TF.to_pil_image(pred_image.add(1).div(2).clamp(0, 1))\n",
    "                if j % args.display_rate == 0 or cur_t == -1:\n",
    "                    image.save(f'{outDirPath}/{filename}')\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02a2872ef89789832e0a654d6c95a175dab3d7e4133113b4cef309e372e0ba06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
